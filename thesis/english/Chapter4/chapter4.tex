\chapter{Proposed Method}
\graphicspath{{Chapter4/Figs/}}

\begin{chapabstract}
This chapter describes the issues of using imbalanced Dataset, feature extraction methods, the proposed Gradient Boosting Decision Trees model, and the reasoning behind.
\end{chapabstract}

\section{Issues of using Imbalanced Dataset}

Most of the related works use the imbalanced dataset \cite{saxe2015deep,vu2017metamorphic}. 
For examples, Saxe and Berlin used the dataset of 431,926 binaries which consists 350,016 malicious files \cite{saxe2015deep}, Vu Thanh Nguyen et al. used the dataset of 9690 files which only has 300 benign files \cite{vu2017metamorphic}. 
In fact, the number of malicious files is often much more massive than the number of benign files because almost benign binaries are often protected by the copyright laws which do not allow for sharing. 
This makes malware identification problem become different from other machine learning classification problems, which commonly have fewer samples in important classes. 
Furthermore, the size of the dataset is usually not large enough because the malware analysis and data labeling are consuming processes that required well-trained security engineers. 
There are also many risks in publishing a large dataset that includes malicious binaries. 

Using imbalanced datasets can make validation metrics misleading.
For examples, with 96.9\% of data is malicious files, a model that labels all samples as malware achieves 96.9\% accuracy, 96.9\% precision (P), 100\% recall (R) and 0.9843 F-score ($F =  2PR / (P + R) $ \cite{chinchor1992muc}).
It also gives way to false positives, which cause negative user experiences.
According to a survey of IT administrators in 2017, 42 percent of companies assume that their users lost productivity as an issue of false-positive results, which creates a choke point for IT administrators in the business life cycle \cite{jonathan2017survey}.


\section{Feature Extraction}
\label{sec:feature-extraction}

By using the simple feature extraction methods inspired by the EMBER dataset owners rather than raw binary files, collecting data is not affected by privacy policies and it is much easier to get a balanced dataset.
By conducting many experiments, we decrease the feature dimension by 30 percent (1711 instead of 2351) to reduce the training time but still manage to achieve a better evaluation result.
In detail, we extracted each Portable Executable file into eight feature groups which can be classified into two types: format-agnostic features and parsed PE features.
File-format agnostic feature groups decrease privacy concerns while parsed PE feature groups encapsulates the information related to executable code.

\subsection{Format-agnostic Features}

We use three groups of features to model the contents of input files in a file-format agnostic way, meaning that it has no dependencies on its format.

\subsubsection{Byte-Entropy Histogram}

The work \cite{saxe2015deep} shows that, in practice, representing byte values in the entropy "context", in which they occur, separates byte values from the context of effectively. 
To compute the byte-entropy histogram, we slide a 2048-length window over all the input bytes with a step size of 1024 bytes. 
Use a simple trick to calculate the entropy $H$ faster, i.e., reducing the information by half, and pairing it with each byte within the window. 
Then, we compute a two-dimensional histogram with $16 \times 16$ bins that quantize entropy and the byte value. 
Finally, we concatenate each row vector in the matrix and normalize the final 256-value vector.

\subsubsection{Byte Histogram}

The byte histogram is a 256-value vector which represents the distribution of each byte value within the file.

\subsubsection{String Information}

The final format-agnostic group of features is string information. These features are derived from the printable sequences of characters in the range \verb|0x20| to \verb|0x7f|, that have at least five characters long. We use the number of strings, the average length of these strings, the amounts of lines  that may sequentially indicate a path (begin with \verb|C:\|), an URL(start with \verb|http://| or \verb|https://|), a registry key (the occurrences of \verb|HKEY_|) and a bundled executable (the short string \verb|MZ|). Also, we use a histogram of the printable characters within these strings.

\subsection{Parsed Features}

In addition to using three format-agnostic groups of features, we extract five other groups from parsing the Portable Executable file by using LIEF - Library to Instrument Executable Formats \cite{lief}.

\subsubsection{General Information}

The set of features includes the file size and necessary information collected from the PE header: the virtual size of the file, the number of imported and exported functions, the number of symbols, whether the data has a debug section, thread local storage, resources, relocations, or a signature.

\subsubsection{Header Information}

We use the information from the Common Object File Format (COFF) header including the timestamp in the header, the target machine and a list of image characteristics. And from the optional header, we use the target subsystem, DLL  characteristics,  the file magic as a string, major and minor image versions, linker versions, system versions and subsystem versions, and the code size, header size and commit size. We use hashing trick with 10 bins for string features \cite{weinberger2009feature}.

\subsubsection{Imported Functions}
\label{sssec:imported}

Parsing the import address table gives us a report about the imported functions by libraries. We use the set of unique libraries with 128-bin hashing trick. Similarly, we apply the 512-bin hashing trick to capture individual functions, by representing
it in string format \verb|library:function|, for example, \verb|kernel32.dll:CreateFileMappingA|.

\subsubsection{Exported Functions}

Similar to extracting imported functions, we summarize a list of the exported functions into a 128-value vector by hashing.

\subsubsection{Section Information}

Properties of each section are used: the name, size, entropy, virtual size, and a list of strings representing section characteristics. We still use the hashing trick on \verb|(section name, value)| pairs to create 50-value vectors containing section size, section entropy, virtual size, and information about entry point characteristics.

\section{Classification}
 
For classification, in this study, we use the Gradient Boosting Decision Trees algorithm with 400 iterations and 64 leaves in one tree. We configure that there must be at least 200 samples in one child, and set learning rate at 5 percent. We elaborate on the reasoning behind these choices below.

Firstly, the massive number of features causes scalability issues for many machine learning algorithms.
 For example, non-linear SVM kernels require $O(N^2)$ multiplication during each iteration, and k-Nearest Neighbors (k-NN) requires significant computation and storage of all label samples during prediction. Accordingly, we target to use neural networks and ensemble decision trees, which are scalable alternatives.

Secondly, our resources, primarily financial support, are lacking. 
But the cost for training neural networks is extremely computationally expensive. 
We tried with some complex models, and these take many hours and require costly more GPUs for speeding. 
Also, neural networks because they are the black boxes that requires much experience to optimize.

Besides, another scalable alternative, tree ensemble algorithms handle very well high dimensional feature spaces as well as a large number of training examples. The two most popular algorithms are Random Forests and Gradient Boosting Decision Trees (GDBT). GBDT training usually takes longer because trees are built sequentially.