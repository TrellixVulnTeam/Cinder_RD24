\chapter{Phương Pháp Đề Xuất}
\graphicspath{{Chapter4/Figs/}}

\begin{chapabstract}
Chương này mô tả các vấn đề về việc sử dụng các tập dữ liệu mất cân bằng, các phương pháp trích xuất đặc trung, mô hình Gradient Boosting Decision Trees được đề xuất và lý do đằng sau.
\end{chapabstract}

\section{Các vấn đề về sử dụng tập dữ liệu không cân bằng}

Hầu hết các nghiên cứu liên quan sử dụng bộ dữ liệu mất cân bằng \cite{saxe2015deep,vu2017metamorphic}. 
Ví dụ, Saxe và Berlin đã sử dụng tập dữ liệu của 431.926 tệp nhị phân bao gồm 350.016 tệp độc hại \cite{saxe2015deep}, Vũ Thanh Nguyên và các cộng sự đã sử dụng tập dữ liệu của 9690 tệp nhưng chỉ có 300 tệp lành tính \cite{vu2017metamorphic}. 
Trên thực tế, số lượng tệp độc hại thường lớn hơn nhiều so với số lượng tệp lành tính vì hầu hết các tệp nhị phân lành tính thường được bảo vệ bởi luật bản quyền không cho phép chia sẻ.
Điều này làm cho vấn đề nhận diện phần mềm độc hại trở nên khác với các vấn đề phân loại học máy khác vì thường có ít mẫu hơn trong các lớp quan trọng.
Hơn nữa, kích thước của tập dữ liệu thường không đủ lớn vì việc phân tích phần mềm độc hại và ghi nhãn dữ liệu là các quy trình tốn thời gian và yêu cầu các kỹ sư bảo mật được đào tạo tốt.
Bên cạnh đó, cũng có nhiều rủi ro trong việc xuất bản tập dữ liệu lớn bao gồm các tệp nhị phân độc hại.

Sử dụng bộ dữ liệu không cân bằng có thể làm cho các số liệu xác thực gây hiểu lầm.
Ví dụ, với 96.9\% dữ liệu là các tệp độc hại, một mô hình đánh dấu tất cả các mẫu là phần mềm độc hại đạt được độ chính xác 96.9\% accuracy, 96.9\% precision (P), 100\% recall (R) và 0.9843 F-score ($F =  2PR / (P + R) $ \cite{chinchor1992muc}).
Nó cũng nhường chỗ cho những dự báo sai, gây ra trải nghiệm người dùng tiêu cực.
Theo một cuộc khảo sát của các quản trị viên CNTT trong năm 2017, 42\% các công ty cho rằng người dùng của họ bị mất năng suất là một vấn đề của kết quả dương tính giả, tạo ra một điểm nghẹt thở cho các quản trị viên CNTT trong môi trường doanh nghiệp \cite{jonathan2017survey}.

\section{Feature Extraction}
\label{sec:feature-extraction}

Bằng cách sử dụng các phương pháp trích xuất đặc trưng đơn giản chứ không phải tệp nhị phân thô, việc thu thập dữ liệu không bị ảnh hưởng bởi chính sách bảo mật và dễ dàng hơn để có được tập dữ liệu cân bằng.
Bằng cách thực hiện nhiều thử nghiệm, chúng tôi giảm kích thước đặc trưng xuống 30\% (1711 thay vì 2351) để giảm thời gian đào tạo nhưng vẫn đạt được kết quả đánh giá tốt hơn.
Cụ thể, chúng tôi trích xuất từng tệp PE thành tám nhóm đặc trưng có thể được phân loại thành hai loại: đặc trưng định dạng bất khả tri (format-agnostic features) và đặc trưng PE được phân tích cú pháp (parsed PE features).
Các nhóm Format-agnostic Feature làm giảm mối quan tâm về quyền riêng tư trong khi các nhóm parsed PE feature đóng gói thông tin liên quan đến mã thực thi.

\subsection{Format-agnostic Features}

Chúng tôi sử dụng ba nhóm đặc trưng để mô hình hóa nội dung của tệp đầu vào độc lập theo định dạng tệp, có nghĩa là nó không phụ thuộc vào định dạng của tệp.

\subsubsection{Byte-Entropy Histogram}

Dựa trên những nghiên cứu của Joshua Saxe và Konstantin Berlin \cite{saxe2015deep}, họ chỉ ra rằng, trong thực tế, hiệu quả của việc tái thể hiện byte value trong ngữ cảnh entropy mà nó xảy ra sẽ phân tách các byte value ra khỏi bối cảnh chung, ví dụ, dữ liệu x86 instruction data  được phân tách từ dữ liệu nén.

Để tính byte-entropy histogram, chúng tôi trượt một cửa sổ 2048-length trên toàn bộ byte đầu vào với bước nhảy 1024 byte. Sử dụng một mẹo đơn giản để tính toán entropy $H$ nhanh hơn, cụ thể, giảm đi một nữa lượng thông tin, và bắt cặp nó với từng byte trong window. Sau đó, chúng tôi tính một histogram 2 chiều với $16 \times 16$ bins chứa entropy và byte value. Cuối cùng, chúng tôi cộng các vector hàng trong ma trận và chuẩn hóa để có một 256-value vector.

\subsubsection{Byte Histogram}

Byte histogram là một 256-value vector đại diện cho phân phối của từng giá trị byte trong tệp.

\subsubsection{String Information}

Nhóm các đặc trưng định dạng bất khả tri cuối cùng là thông tin chuỗi. Các đặc trưng này được trích xuất từ những chuỗi các kí tự in được trong khoảng \verb|0x20| đến \verb|0x7f|, và phải có ít nhất 5 kí tự. Chúng tôi sử dụng số lượng chuỗi, độ dài trung bình của các chuỗi, số lượng các chuỗi có khả năng là đường dẫn (bắt đầu với \verb|C:\|), địa chỉ web (bắt đầu với \verb|http://| hoặc \verb|https://|), a registry key (sự xuất hiện của \verb|HKEY_|) và một file thực thi được nén (chuỗi \verb|MZ|). Ngoài ra, chúng tôi sử dụng histogram của các ký tự có thể in trong các chuỗi này.

\subsection{Parsed Features}

Ngoài việc sử dụng ba nhóm đặc trưng định dạng bất khả tri, chúng tôi trích xuất năm nhóm khác từ phân tích tệp PE bằng cách sử dụng LIEF - Library to Instrument Executable Formats \cite{lief}.

\subsubsection{General Information}

Đây là tập hợp các đặc trưng bao gồm file size và những thông tin cần thiết khác từ PE header:  virtual size, số lượng imported và exported functions, số lượng symbols, dữ liệu có hay không debug section, thread local storage, resources, relocations, hoặc signature.

\subsubsection{Header Information}

Chúng tôi sử dụng thông tinh từ Common Object File Format (COFF) header bao gồm timestamp, target machine và danh sách image characteristics. Từ optional header, chúng tôi dùng target subsystem, DLL characteristics, file magic, major và minor image versions, linker versions, system versions và subsystem versions, code size, header size và commit size. Chúng tôi sử dụng hashing trick với 10 bins có các đặc trưng dạng chuỗi \cite{weinberger2009feature}.

\subsubsection{Imported Functions}
\label{sssec:imported}

Phân tích import address table cho chúng ta một báo cáo về các hàm được import bởi các thư viện.
Chúng tôi sử dụng tập hợp các thư viện với hashing trick 128-bin và áp dụng hashing 512-bin để ghi lại các function riêng lẻ, bằng cách biểu diễn dưới định dạng chuỗi \verb|library:function|, ví dụ, \verb|kernel32.dll:CreateFileMappingA|.

\subsubsection{Exported Functions}

Tương tự như cách trích xuất imported function, chúng tôi thống kê một danh sách các exported function vào một 128-value vector bằng cách hashing.

\subsubsection{Section Information}

Thuộc tính của từng section được sử dụng: name, size, entropy, virtual size, và một danh sách các chuỗi thể hiện characteristics của section.Chúng tôi vẫn sử dụng hashing trick trên các cặp \verb|(section name, value)| để tạo ra những 50-value vector chứa section size, section entropy, virtual size, và thông tin về entry point characteristics.

\section{Classification}
 
Trong nghiên cứu này, we đề xuất sử dụng thuật toán Gradient Boosting Decision Trees với 400 iterations và 64 lá mỗi cây. Chúng tôi cấu hình rằng phải có ít nhất 200 mẫu ở một nút và đặt tỷ lệ học tập là 5 phần trăm. Chúng tôi giải thích về lý do đằng sau những lựa chọn dưới đây.

Thứ nhất, số lượng lớn các đặc trưng gây ra các vấn đề về khả năng mở rộng cho nhiều thuật toán học máy. Ví dụ, non-linear SVM kernels yêu cầu $O(N^2)$ phép nhân trong mỗi lần lặp, và k-Nearest Neighbors (k-NN) yêu cầu một lượng tính toán đáng kể và lưu trữ tất cả các mẫu nhãn trong khi dự đoán. Do đó, chúng tôi tập trung vào việc sử dụng neural networks và ensemble decision trees, đó là lựa chọn thay thế có thể mở rộng.

Thứ hai, nguồn lực của chúng tôi, chủ yếu là hỗ trợ tài chính, rất ít. But chi phí cho việc đào tạo neural networks cực kỳ tốn kém tính toán. Các mô hình phức tạp mất nhiều giờ và đòi hỏi nhiều GPU hơn để tăng tốc. Ngoài ra, neural networks là hộp đen và yêu cầu nhiều kinh nghiệm để tối ưu hóa.

Bên cạnh đó, các thuật toán tree ensemble xử lý các không gian đặc trưng lớn rất tốt, cũng như xử lí tốt một số lượng lớn các mẫu đào tạo. Hai thuật toán phổ biến là Random Forests và Gradient Boosting Decision Trees (GDBT). Đào tạo GBDT thường mất nhiều thời gian hơn vì cây được xây dựng theo tuần tự. Tuy nhiên, kết quả cho thấy GBDT tốt hơn so với Random Forests.
