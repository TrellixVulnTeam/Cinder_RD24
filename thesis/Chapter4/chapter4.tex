\chapter{Proposed Method}
\graphicspath{{Chapter4/Figs/}}

\begin{chapabstract}
This chapter describes how to extract features from Portable Executable files, the Gradient Boosting Decision Trees model we used, and the reasoning behind.
\end{chapabstract}

\section{Feature Extraction}
\label{sec:feature-extraction}

Inspired by Hyrum S. Anderson and Phil Roth from Endgame, Inc. \cite{anderson2018ember}, we extracted each PE file into eight feature groups which can be classified into two types: format-agnostic features and parsed features.

\subsection{Format-agnostic Features}

We use three groups of features to model the contents of input files in a file-format agnostic way, meaning that it has no dependencies on its format. By providing the simple statistical summaries rather than a listing of raw features, e.g., all bytes or all strings, we decrease privacy concerns and are easy to request or publish the dataset.

\subsubsection{Byte-Entropy Histogram}

Based on the work published by Joshua Saxe and Konstantin Berlin \cite{saxe2015deep}, they show that, in practice, the effect of representing byte values in the entropy context in which they occur separates byte values that arise in the context of, for example, x86 instruction data from, for example, byte values occurring in compressed data.

To compute the byte-entropy histogram, we slide a 2048-length window over all the input bytes with a step size of 1024 bytes. Use a simple trick to calculate the entropy $H$ faster, i.e., reducing the information by half, and pairing it with each byte within the window. Then, we compute a two-dimensional histogram with $16 \times 16$ bins that quantize entropy and the byte value. Finally, we concatenate each row vector in the matrix and normalize the final 256-value vector.

\subsubsection{Byte Histogram}

The byte histogram is a 256-value vector which represents the distribution of each byte value within the file.

\subsubsection{String Information}

The final format-agnostic group of features is string information. These features are derived from the printable sequences of characters in the range \verb|0x20| to \verb|0x7f|, that have at least five characters long. We use the number of strings, the average length of these strings, the amounts of lines  that may sequentially indicate a path (begin with \verb|C:\|), an URL(start with \verb|http://| or \verb|https://|), a registry key (the occurrences of \verb|HKEY_|) and a bundled executable (the short string \verb|MZ|). Also, we use a histogram of the printable characters within these strings.

\subsection{Parsed Features}

In addition to using three format-agnostic groups of features, we extract five other groups from parsing the Portable Executable file by using LIEF - Library to Instrument Executable Formats \cite{lief}.

\subsubsection{General Information}

The set of features includes the file size and necessary information collected from the PE header: the virtual size of the file, the number of imported and exported functions, the number of symbols, whether the data has a debug section, thread local storage, resources, relocations, or a signature.

\subsubsection{Header Information}

We use the information from the Common Object File Format (COFF) header including the timestamp in the header, the target machine and a list of image characteristics. And from the optional header, we use the target subsystem, DLL  characteristics,  the file magic as a string, major and minor image versions, linker versions, system versions and subsystem versions, and the code size, header size and commit size. We use hashing trick with 10 bins for string features \cite{weinberger2009feature}.

\subsubsection{Imported Functions}
\label{sssec:imported}

Parsing the import address table gives us a report about the imported functions by libraries. We use the set of unique libraries with 128-bin hashing trick. Similarly, we apply the 512-bin hashing trick to capture individual functions, by representing
it in string format \verb|library:function|, for example, \verb|kernel32.dll:CreateFileMappingA|.

\subsubsection{Exported Functions}

Similar to extracting imported functions, we summarize a list of the exported functions into a 128-value vector by hashing.

\subsubsection{Section Information}

Properties of each section are used: the name, size, entropy, virtual size, and a list of strings representing section characteristics. We still use the hashing trick on \verb|(section name, value)| pairs to create 50-value vectors containing section size, section entropy, virtual size, and information about entry point characteristics.

\section{Classification}
 
For classification, in this study, we use the Gradient Boosting Decision Trees algorithm with 400 iterations and 64 leaves in one tree. We configure that there must be at least 200 samples in one child, and set learning rate at 5 percent. We elaborate on the reasoning behind these choices below.

Firstly, the massive number of features causes scalability issues for many machine learning algorithms.  For example,  non-linear  SVM  kernels require $O(N^2)$ multiplication during each iteration, and k-Nearest Neighbors (k-NN) requires significant computation and storage of all label samples during prediction. Accordingly, we target to use neural networks and ensemble decision trees, which are scalable alternatives.

Secondly, our resources, primarily financial support, are lacking. But the cost for training neural networks is extremely computationally expensive. We tried with some complex models, and these take many hours and require costly more GPUs for speeding. Also, neural networks are the black-box that not much can be gleaned from and required much experience to optimize.

Besides, another scalable alternative, tree ensemble algorithms handle very well high dimensional feature spaces as well as a large number of training examples. The two most popular algorithms are Random Forests and Gradient Boosting Decision Trees (GDBT). GBDT training usually takes longer because trees are built sequentially. However, benchmark results have shown GBDT are better learners than Random Forests.

In practice, in our experiments, Gradient Boosting Decision Trees algorithm always takes less training time in comparing to neural networks with the same hardwares.
